{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI-Powered Scenario Optimization: Warm-up\n",
    "\n",
    "## Introduction\n",
    "\n",
    "\n",
    "In this hackathon, you'll be developing an AI-powered solution to optimize manufacturing processes for sustainability. Specifically, you'll be working with HappyBikes, a company that produces traditional and electronic bicycles, to optimize their manufacturing processes for cost and environmental impact.\n",
    "\n",
    "This notebook will introduce you to the key optimization techniques.\n",
    "\n",
    "It covers:\n",
    "1. The optimization problem overview\n",
    "2. Introduction to Genetic Algorithms\n",
    "3. Introduction to Particle Swarm Optimization\n",
    "4. How to apply these techniques to the bicycle manufacturing problem\n",
    "5. Simple examples to illustrate the concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Optimization Problem Overview\n",
    "\n",
    "### The Challenge\n",
    "\n",
    "HappyBikes produces traditional and electronic bicycles, each composed of multiple components (saddles, wheels, brakes, etc.). Each component can be manufactured using different materials and production methods (\"recipes\").\n",
    "\n",
    "Different \"recipes\" lead to varying:\n",
    "- Production costs\n",
    "- CO₂ emissions\n",
    "- Water consumption\n",
    "- Product durability\n",
    "\n",
    "### Optimization Goals\n",
    "\n",
    "- **Primary Objectives**: Minimize costs AND minimize CO₂ emissions\n",
    "- **Secondary Objectives**: When primary metrics are tied, optimize for:\n",
    "  - Lower water consumption\n",
    "  - Higher product durability (calculated as average of materials durability)\n",
    "\n",
    "### Constraints\n",
    "\n",
    "The optimization is subject to various constraints, such as:\n",
    "- Material-based constraints (e.g., maximum 5kg of copper)\n",
    "- Parameter-based constraints (e.g., minimum durability of 5 months)\n",
    "- Cost constraints (e.g., maximum cost of $100)\n",
    "\n",
    "This is a **multi-objective optimization problem** with **constraints**, which makes it challenging to solve using traditional optimization methods. This is where AI-powered optimization techniques come in!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Introduction to Genetic Algorithms\n",
    "\n",
    "### What are Genetic Algorithms?\n",
    "\n",
    "Genetic Algorithms (GAs) are optimization algorithms inspired by the process of natural selection. They work by evolving a population of candidate solutions over several generations with the aim of improving their fitness to solve a specific problem. They are particularly well-suited for complex optimization problems where the search space is large and potentially discontinuous.\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "1. **Chromosome/Individual**: A potential solution to the problem, represented as a string of genes.\n",
    "2. **Population**: A collection of individuals (potential solutions).\n",
    "3. **Fitness Function**: A function that evaluates how good a solution is.\n",
    "4. **Selection**: The process of choosing individuals for reproduction based on their fitness.\n",
    "5. **Crossover**: The process of combining genetic material from two parents to create offspring.\n",
    "6. **Mutation**: Random changes to individuals to maintain genetic diversity.\n",
    "\n",
    "### The Genetic Algorithm Process\n",
    "\n",
    "1. **Initialization**: Create an initial population of (random) solutions.\n",
    "2. **Evaluation**: Calculate the fitness of each individual in the population.\n",
    "3. **Selection**: Select individuals for reproduction based on their fitness.\n",
    "4. **Crossover**: Create new individuals by combining genetic material from selected parents.\n",
    "5. **Mutation**: Introduce random changes to some individuals to maintain genetic diversity.\n",
    "6. **Replacement**: Replace the old population with the new population.\n",
    "7. **Termination**: Repeat steps 2-6 until a termination condition is met (e.g., maximum number of generations, satisfactory solution found)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Example of a Genetic Algorithm\n",
    "\n",
    "Let's implement a simple genetic algorithm to find the maximum value of the function $f(x) = x^2$ in the range $[0, 31]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4469378",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "\n",
    "# Define the fitness function\n",
    "def fitness_function(x):\n",
    "    return x**2\n",
    "\n",
    "\n",
    "# Convert binary string to integer\n",
    "def binary_to_int(binary_string):\n",
    "    return int(binary_string, 2)\n",
    "\n",
    "\n",
    "# Generate a random individual (binary string of length 5)\n",
    "def generate_individual():\n",
    "    return \"\".join(random.choice(\"01\") for _ in range(5))\n",
    "\n",
    "\n",
    "# Generate initial population\n",
    "def generate_population(size):\n",
    "    return [generate_individual() for _ in range(size)]\n",
    "\n",
    "\n",
    "# Select individual with the highest fitness score for reproduction using tournament selection\n",
    "def selection(population, fitnesses, k=3):\n",
    "    # Select k random individuals from the population\n",
    "    selected_indices = random.sample(range(len(population)), k)\n",
    "    # Evaluate their fitness\n",
    "    selected_fitnesses = [fitnesses[i] for i in selected_indices]\n",
    "    # Select the individual with the highest fitness\n",
    "    return population[selected_indices[np.argmax(selected_fitnesses)]]\n",
    "\n",
    "\n",
    "# Perform single-point crossover\n",
    "def crossover(parent1, parent2):\n",
    "    # Choose a random crossover point\n",
    "    crossover_point = random.randint(1, len(parent1) - 1)\n",
    "    # Create offspring by combining parts of parents\n",
    "    child1 = parent1[:crossover_point] + parent2[crossover_point:]\n",
    "    child2 = parent2[:crossover_point] + parent1[crossover_point:]\n",
    "    return child1, child2\n",
    "\n",
    "\n",
    "# Perform mutation\n",
    "def mutation(individual, mutation_rate=0.1):\n",
    "    # Flip each bit with probability mutation_rate\n",
    "    mutated = \"\"\n",
    "    for bit in individual:\n",
    "        if random.random() < mutation_rate:\n",
    "            mutated += \"1\" if bit == \"0\" else \"0\"\n",
    "        else:\n",
    "            mutated += bit\n",
    "    return mutated\n",
    "\n",
    "\n",
    "# Run the genetic algorithm\n",
    "def genetic_algorithm(population_size=5, generations=10):\n",
    "    # Generate initial population\n",
    "    population = generate_population(population_size)\n",
    "\n",
    "    # Keep track of the best solution found so far\n",
    "    best_individual = None\n",
    "    best_fitness = -float(\"inf\")\n",
    "\n",
    "    # Keep track of the average fitness per generation\n",
    "    avg_fitness_history = []\n",
    "    best_fitness_history = []\n",
    "\n",
    "    # Run for a fixed number of generations\n",
    "    for generation in range(generations):\n",
    "        # Evaluate fitness of each individual\n",
    "        fitnesses = np.array(\n",
    "            [fitness_function(binary_to_int(ind)) for ind in population]\n",
    "        )\n",
    "\n",
    "        # Update best solution\n",
    "        max_fitness_idx = np.argmax(fitnesses)\n",
    "        if fitnesses[max_fitness_idx] > best_fitness:\n",
    "            best_individual = population[max_fitness_idx]\n",
    "            best_fitness = fitnesses[max_fitness_idx]\n",
    "\n",
    "        # Record statistics\n",
    "        avg_fitness_history.append(np.mean(fitnesses))\n",
    "        best_fitness_history.append(best_fitness)\n",
    "\n",
    "        # Create new population\n",
    "        new_population = []\n",
    "\n",
    "        # Elitism: keep the best individual\n",
    "        new_population.append(population[max_fitness_idx])\n",
    "\n",
    "        # Create the rest of the new population\n",
    "        while len(new_population) < population_size:\n",
    "            # Select parents\n",
    "            parent1 = selection(np.array(population), fitnesses)\n",
    "            parent2 = selection(np.array(population), fitnesses)\n",
    "\n",
    "            # Perform crossover\n",
    "            child1, child2 = crossover(parent1, parent2)\n",
    "\n",
    "            # Perform mutation\n",
    "            child1 = mutation(child1)\n",
    "            child2 = mutation(child2)\n",
    "\n",
    "            # Add children to new population\n",
    "            new_population.append(child1)\n",
    "            if len(new_population) < population_size:\n",
    "                new_population.append(child2)\n",
    "\n",
    "        # Replace old population with new population\n",
    "        population = new_population\n",
    "\n",
    "        print(f\"Generation {generation + 1}:\")\n",
    "        print(f\"Best individual: {best_individual}\")\n",
    "        print(f\"Best individual (decimal): {binary_to_int(best_individual)}\")\n",
    "        print(f\"Best fitness: {best_fitness}\")\n",
    "        print(\"--------------------------------------------------\")\n",
    "\n",
    "    return best_individual, best_fitness, avg_fitness_history, best_fitness_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7e6ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the genetic algorithm\n",
    "best_individual, best_fitness, avg_fitness_history, best_fitness_history = (\n",
    "    genetic_algorithm()\n",
    ")\n",
    "\n",
    "# Plot fitness history\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(avg_fitness_history, label=\"Average Fitness\")\n",
    "plt.plot(best_fitness_history, label=\"Best Fitness\")\n",
    "plt.xlabel(\"Generation\")\n",
    "plt.ylabel(\"Fitness\")\n",
    "plt.title(\"Fitness History\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you launch above cell multiple times you will see that the process is not deterministic and not always the best solution is found. \n",
    "\n",
    "However, always the GA tries to maximize the best solution and goes in correct direction.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Introduction to Particle Swarm Optimization\n",
    "\n",
    "### What is Particle Swarm Optimization?\n",
    "\n",
    "Particle Swarm Optimization (PSO) is another population-based optimization algorithm, inspired by the social behavior of birds flocking or fish schooling. Unlike genetic algorithms, PSO doesn't use evolutionary operators like crossover and mutation. Instead, it uses the concept of \"particles\" moving through the search space, guided by their own best known position and the best known position of the entire swarm.\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "1. **Particle**: A potential solution to the problem, represented as a position in the search space.\n",
    "2. **Velocity**: The rate at which a particle changes its position.\n",
    "3. **Personal Best (pbest)**: The best position a particle has found so far.\n",
    "4. **Global Best (gbest)**: The best position found by any particle in the swarm.\n",
    "5. **Inertia Weight**: Controls the impact of the previous velocity on the current velocity.\n",
    "6. **Cognitive Component**: Controls the influence of the particle's personal best position.\n",
    "7. **Social Component**: Controls the influence of the global best position.\n",
    "\n",
    "### The PSO Process\n",
    "\n",
    "1. **Initialization**: Create a swarm of particles with random positions and velocities.\n",
    "2. **Evaluation**: Calculate the fitness of each particle.\n",
    "3. **Update Personal Best**: If a particle's current position is better than its personal best, update its personal best.\n",
    "4. **Update Global Best**: If a particle's current position is better than the global best, update the global best.\n",
    "5. **Update Velocity and Position**: Update each particle's velocity and position based on its previous velocity, personal best, and global best.\n",
    "6. **Termination**: Repeat steps 2-5 until a termination condition is met."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64258e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple objective function to minimize: f(x,y) = x^2 + y^2\n",
    "# This is a simple bowl-shaped function with minimum at (0,0)\n",
    "def objective_function(position):\n",
    "    \"\"\"\n",
    "    Our objective function that we want to minimize.\n",
    "    In this case, it's a simple bowl-shaped function.\n",
    "    \"\"\"\n",
    "    x, y = position  # Unpack the position array to x and y values\n",
    "    return x**2 + y**2  # Calculate x^2 + y^2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ca7c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Particle:\n",
    "    \"\"\"\n",
    "    A single particle in the swarm.\n",
    "    Each particle represents a potential solution to our problem.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, bounds):\n",
    "        \"\"\"\n",
    "        Initialize a new particle with random position and velocity.\n",
    "\n",
    "        Args:\n",
    "            bounds: List of tuples defining the search space [(x_min, x_max), (y_min, y_max)]\n",
    "        \"\"\"\n",
    "        # Initialize random position within the specified bounds\n",
    "        self.position = np.array(\n",
    "            [\n",
    "                np.random.uniform(bounds[0][0], bounds[0][1]),\n",
    "                np.random.uniform(bounds[1][0], bounds[1][1]),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Initialize random velocity\n",
    "        self.velocity = np.random.uniform(\n",
    "            -0.5, 0.5, 2\n",
    "        )  # 2-dimensional velocity for x and y from range -0.5 to 0.5\n",
    "\n",
    "        # Initialize personal best position to current position\n",
    "        self.best_position = self.position.copy()\n",
    "\n",
    "        # Initialize best score (function value) to infinity (since we're minimizing)\n",
    "        self.best_score = float(\"inf\")\n",
    "\n",
    "    def update_personal_best(self):\n",
    "        \"\"\"Update the particle's personal best position if current position is better.\"\"\"\n",
    "        # Calculate current score (function value) at current position\n",
    "        current_score = objective_function(self.position)\n",
    "\n",
    "        # If current score is better than best score ever found by this particle\n",
    "        if current_score < self.best_score:\n",
    "            # Update personal best position and score\n",
    "            self.best_position = self.position.copy()\n",
    "            self.best_score = current_score\n",
    "\n",
    "    def update_velocity(self, global_best_position, w=0.5, c1=1.5, c2=1.5):\n",
    "        \"\"\"\n",
    "        Update the particle's velocity based on inertia, cognitive component, and social component.\n",
    "\n",
    "        Args:\n",
    "            global_best_position: Best position found by any particle in the swarm\n",
    "            w: Inertia weight - controls impact of previous velocity\n",
    "            c1: Cognitive weight - controls particle's tendency to return to personal best\n",
    "            c2: Social weight - controls particle's tendency to move toward swarm's best\n",
    "        \"\"\"\n",
    "        # Random factors for cognitive and social components\n",
    "        r1 = np.random.random(2)\n",
    "        r2 = np.random.random(2)\n",
    "\n",
    "        # Inertia component - tendency to continue moving in same direction\n",
    "        inertia = w * self.velocity\n",
    "\n",
    "        # Cognitive component - attraction to particle's personal best position\n",
    "        cognitive_component = c1 * r1 * (self.best_position - self.position)\n",
    "\n",
    "        # Social component - attraction to swarm's global best position\n",
    "        social_component = c2 * r2 * (global_best_position - self.position)\n",
    "\n",
    "        # Update velocity as sum of these components\n",
    "        self.velocity = inertia + cognitive_component + social_component\n",
    "\n",
    "    def update_position(self, bounds):\n",
    "        \"\"\"\n",
    "        Update the particle's position based on its velocity and ensure it stays within bounds.\n",
    "\n",
    "        Args:\n",
    "            bounds: List of tuples defining the search space [(x_min, x_max), (y_min, y_max)]\n",
    "        \"\"\"\n",
    "        # Update position by adding velocity\n",
    "        self.position = self.position + self.velocity\n",
    "\n",
    "        # Ensure position stays within bounds\n",
    "        for i in range(len(bounds)):\n",
    "            if self.position[i] < bounds[i][0]:\n",
    "                self.position[i] = bounds[i][0]\n",
    "            elif self.position[i] > bounds[i][1]:\n",
    "                self.position[i] = bounds[i][1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67cca91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def particle_swarm_optimization(\n",
    "    num_particles=20, max_iterations=100, bounds=[(-10, 10), (-10, 10)]\n",
    "):\n",
    "    \"\"\"\n",
    "    Main PSO algorithm.\n",
    "\n",
    "    Args:\n",
    "        num_particles: Number of particles in the swarm\n",
    "        max_iterations: Maximum number of iterations\n",
    "        bounds: Search space bounds for each dimension\n",
    "\n",
    "    Returns:\n",
    "        global_best_position: The best solution found\n",
    "        global_best_score: The score (function value) of the best solution\n",
    "        position_history: History of global best positions for visualization\n",
    "    \"\"\"\n",
    "    # Initialize a swarm of particles\n",
    "    swarm = [Particle(bounds) for _ in range(num_particles)]\n",
    "\n",
    "    # Initialize global best position and score\n",
    "    global_best_position = None\n",
    "    global_best_score = float(\"inf\")\n",
    "\n",
    "    # Create a list to store history of global best positions (for visualization)\n",
    "    position_history = []\n",
    "\n",
    "    # Main PSO loop\n",
    "    for iteration in range(max_iterations):\n",
    "        # For each particle in the swarm\n",
    "        for particle in swarm:\n",
    "            # Calculate current score (function value) at particle's position\n",
    "            current_score = objective_function(particle.position)\n",
    "\n",
    "            # Update global best if this particle's position is better\n",
    "            if current_score < global_best_score:\n",
    "                global_best_position = particle.position.copy()\n",
    "                global_best_score = current_score\n",
    "\n",
    "                # Add to history for visualization\n",
    "                position_history.append(\n",
    "                    (global_best_position.copy(), global_best_score)\n",
    "                )\n",
    "\n",
    "            # Update particle's personal best\n",
    "            particle.update_personal_best()\n",
    "\n",
    "        # After evaluating all particles, update their velocities and positions\n",
    "        for particle in swarm:\n",
    "            particle.update_velocity(global_best_position)\n",
    "            particle.update_position(bounds)\n",
    "\n",
    "        # Print current best solution (every 10 iterations)\n",
    "        if iteration % 10 == 0:\n",
    "            print(f\"Iteration {iteration}: Best Score = {global_best_score}\")\n",
    "            print(f\"Best Position = {global_best_position}\")\n",
    "\n",
    "    # Print final results\n",
    "    print(\"\\nPSO Optimization Complete!\")\n",
    "    print(f\"Best Solution: {global_best_position}\")\n",
    "    print(f\"Best Score: {global_best_score}\")\n",
    "\n",
    "    return global_best_position, global_best_score, position_history\n",
    "\n",
    "\n",
    "def visualize_result(position_history, bounds):\n",
    "    \"\"\"\n",
    "    Visualize the optimization process and the objective function.\n",
    "\n",
    "    Args:\n",
    "        position_history: History of global best positions during optimization\n",
    "        bounds: Search space bounds\n",
    "    \"\"\"\n",
    "    # Create a grid of points to evaluate the objective function\n",
    "    x = np.linspace(bounds[0][0], bounds[0][1], 100)\n",
    "    y = np.linspace(bounds[1][0], bounds[1][1], 100)\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    Z = np.zeros_like(X)\n",
    "\n",
    "    # Calculate function values for all grid points\n",
    "    for i in range(X.shape[0]):\n",
    "        for j in range(X.shape[1]):\n",
    "            Z[i, j] = objective_function([X[i, j], Y[i, j]])\n",
    "\n",
    "    # Create contour plot of the objective function\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.contourf(X, Y, Z, 50, cmap=\"viridis\", alpha=0.8)\n",
    "    plt.colorbar(label=\"Objective Function Value\")\n",
    "\n",
    "    # Extract positions and scores from history\n",
    "    positions = np.array([p[0] for p in position_history])\n",
    "    scores = np.array([p[1] for p in position_history])\n",
    "\n",
    "    # Plot the particle's path\n",
    "    plt.plot(\n",
    "        positions[:, 0],\n",
    "        positions[:, 1],\n",
    "        \"o-\",\n",
    "        color=\"red\",\n",
    "        markersize=3,\n",
    "        linewidth=1,\n",
    "        label=\"Optimization Path\",\n",
    "    )\n",
    "\n",
    "    # Highlight the final best position\n",
    "    plt.scatter(\n",
    "        positions[-1, 0],\n",
    "        positions[-1, 1],\n",
    "        color=\"white\",\n",
    "        s=100,\n",
    "        edgecolor=\"black\",\n",
    "        label=\"Best Solution\",\n",
    "    )\n",
    "\n",
    "    plt.title(\"Particle Swarm Optimization\")\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(\"y\")\n",
    "    plt.legend()\n",
    "    plt.grid(True, linestyle=\"--\", alpha=0.7)\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b03e15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define search space bounds: x and y are both in range [-10, 10]\n",
    "bounds = [(-10, 10), (-10, 10)]\n",
    "\n",
    "# Run PSO algorithm\n",
    "best_position, best_score, position_history = particle_swarm_optimization(\n",
    "    num_particles=50,  # Number of particles in the swarm\n",
    "    max_iterations=100,  # Number of iterations to run\n",
    "    bounds=bounds,  # Search space bounds\n",
    ")\n",
    "\n",
    "# Visualize the results\n",
    "visualize_result(position_history, bounds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Applying Optimization Techniques to the Bicycle Manufacturing Problem\n",
    "\n",
    "Now that we understand the basics of Genetic Algorithms and Particle Swarm Optimization, let's discuss how these techniques can be applied to the bicycle manufacturing optimization problem.\n",
    "\n",
    "### Problem Representation\n",
    "\n",
    "First, we need to decide how to represent a solution to the problem. In this case, a solution is a specific configuration of components for a bicycle. We can represent this as a vector where each element corresponds to a specific component choice.\n",
    "\n",
    "For example, if we have 5 different types of components (saddle, wheels, brakes, frame, handlebars), and each component has multiple versions (e.g., saddle_A, saddle_B, etc.), we can represent a solution as a vector of indices, where each index corresponds to the chosen version for each component type.\n",
    "\n",
    "### Fitness Function / Objective Function\n",
    "\n",
    "The fitness function (for GA) or objective function (for PSO) should evaluate how good a solution is based on the optimization goals. In this case, we have multiple objectives:\n",
    "\n",
    "1. Minimize production costs\n",
    "2. Minimize CO₂ emissions\n",
    "3. Minimize water consumption (secondary)\n",
    "4. Maximize average product durability (secondary)\n",
    "\n",
    "We can use a weighted sum approach to combine these objectives into a 2-step fitness value: \n",
    "- primary: minimize sum of coats and CO₂ emissions\n",
    "- secondary: miminize sum of all metrics (that requires change of sign of product durability, as this is the only metric we want to maximize).\n",
    "\n",
    "\n",
    "### Handling Constraints\n",
    "\n",
    "Both GA and PSO need to handle constraints such as:\n",
    "- Material-based constraints (e.g., maximum 5kg of copper)\n",
    "- Parameter-based constraints (e.g., minimum durability of 5 months)\n",
    "\n",
    "There are several ways to handle constraints:\n",
    "1. **Penalty Functions**: Add a penalty term to the fitness function for solutions that violate constraints.\n",
    "2. **Repair Mechanisms**: Modify invalid solutions to make them valid.\n",
    "3. **Constrained Optimization**: Use specialized algorithms that directly handle constraints.\n",
    "\n",
    "### Genetic Algorithm Approach\n",
    "\n",
    "For a GA approach to the bicycle manufacturing problem:\n",
    "\n",
    "1. **Representation**: Each individual (chromosome) represents a specific configuration of components for a bicycle.\n",
    "2. **Initialization**: Generate a random population of bicycle configurations.\n",
    "3. **Fitness Evaluation**: Calculate the fitness of each configuration based on cost, CO₂ emissions, water consumption, and durability.\n",
    "4. **Selection**: Select configurations for reproduction based on their fitness.\n",
    "5. **Crossover**: Create new configurations by combining components from selected parents.\n",
    "6. **Mutation**: Introduce random changes to some configurations to maintain diversity.\n",
    "7. **Replacement**: Replace the old population with the new population.\n",
    "8. **Termination**: Repeat steps 3-7 until a termination condition is met.\n",
    "\n",
    "### Particle Swarm Optimization Approach\n",
    "\n",
    "For a PSO approach to the bicycle manufacturing problem:\n",
    "\n",
    "1. **Representation**: Each particle represents a specific configuration of components for a bicycle.\n",
    "2. **Initialization**: Generate a random swarm of bicycle configurations.\n",
    "3. **Fitness Evaluation**: Calculate the fitness of each configuration based on cost, CO₂ emissions, water consumption, and durability.\n",
    "4. **Update Personal Best**: If a configuration's current fitness is better than its personal best, update its personal best.\n",
    "5. **Update Global Best**: If a configuration's current fitness is better than the global best, update the global best.\n",
    "6. **Update Velocity and Position**: Update each configuration's velocity and position based on its previous velocity, personal best, and global best.\n",
    "7. **Termination**: Repeat steps 3-6 until a termination condition is met."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Conclusion\n",
    "\n",
    "In this warm-up notebook, we introduced the optimization problem for the bicycle manufacturing challenge and provided an overview of Genetic Algorithms and Particle Swarm Optimization. We also provided simple examples to illustrate these concepts.\n",
    "\n",
    "In the next steps, you'll be able to apply these techniques to the bicycle manufacturing problem using the provided dataset and optimization framework. Good luck, and happy optimizing!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

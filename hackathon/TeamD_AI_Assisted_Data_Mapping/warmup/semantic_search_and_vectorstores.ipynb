{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Core Semantic Search Concepts\n",
    "\n",
    "This notebook covers the fundamental concepts of semantic search, including vectors, cosine similarity, vector databases, Pinecone implementation, and hybrid search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Vectors\n",
    "\n",
    "Vectors are mathematical representations of data, essentially, a list of numbers. This representation allows us to express different types of data in a unified, mathematical way and apply mathematical tools in order to analyze and manipulate them. For this projetc, the crucial part is the ability to measure 'similarity' between vectors and, conversely, between different pieces of information. For beginners, let's break this down:\n",
    "\n",
    "### What are Vectors?\n",
    "\n",
    "It's useful to think about vectors as points in space (or arrows for more physics-oriented viewers). For example:\n",
    "- A 2D vector [3, 4] represents a point in a flat space, 3 units along the x-axis and 4 units along the y-axis\n",
    "- A 3D vector [1, 2, 3] represents a point in 3D space\n",
    "\n",
    "A visual format helps us understand relationship between vectors. It's not obvious if 2 vectors are 'similar' to each other when looking at a list of numbers. However, when turned into points in space we can simply check how close those vectors are to each other. This 'closeness' represents similarity.\n",
    "\n",
    "While we can easily visualize 2D and 3D vectors as points(arrows) in space, higher dimensional vectors are harder to picture. However, the same principles apply - each number in the vector represents a position along a different dimension.\n",
    "\n",
    "### Why Vectors Matter in Semantic Search\n",
    "\n",
    "The critical feature of a vector is that it can represent \"meaning\". Text, images, sound waves, all can be converted into vectors (with proper tools) that contain similar meaning to the original data. \n",
    "\n",
    "Disclaimer: Vector is just a way to represent data. It can store anything that can be turned into numbers. In terms of text, it's often used to represent, a 'structure' of a text (syntactics), a 'meaning' of a text (semantics) and presence of keywords.\n",
    "\n",
    "Vectors allow us to:\n",
    "1. **Represent meaning mathematically** - Convert words, sentences, images into numbers\n",
    "2. **Measure similarity** - Similar meanings have vectors that are close to each other\n",
    "3. **Perform efficient searches** - Find information based on meaning rather than just keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Representations of Text\n",
    "\n",
    "Text can be converted into vectors using embedding models, which capture semantic meaning. This process is fundamental to semantic search, so let's explore it in details.\n",
    "\n",
    "### What are Text Embeddings?\n",
    "\n",
    "Text embeddings are vector representations of words, sentences, or documents. These vectors capture the meaning of the text in a way that computers can process.\n",
    "\n",
    "For example, the word \"king\" might be represented as a vector like [0.1, -0.2, 0.5, ...], where each number captures some aspect of its meaning. Words with similar meanings will have similar vectors.\n",
    "\n",
    "Very inituive embedding method is **Bag of Words**, which represents a document as a vector of word frequencies. Each dimension in the vector corresponds to a word in the vocabulary, and the value represents the frequency of that word in the document.\n",
    "The order of words in a document is not important, and that the presence or absence of a word is what matters.\n",
    "BoW does not require any training data; it simply counts the frequency of each word in the document. \n",
    "\n",
    "![image](../bow.png)\n",
    "\n",
    "While Bag Of Words is very simple embedding method, the most popular are embedding gerated by Neural Networks. LLMs, such as BERT, RoBERTa, or ada-002, generate contextualized embeddings that capture the nuances of language. These embeddings are learned during the pre-training process and can be fine-tuned for specific downstream tasks. LLM embeddings are highly effective in capturing complex semantic relationships and have achieved state-of-the-art results in many NLP tasks.\n",
    "\n",
    "### Why Do We Need Embeddings?\n",
    "\n",
    "1. **Computers don't understand text directly** - They need numerical representations to process language\n",
    "2. **Semantic relationships become mathematical** - Words like \"king\" and \"queen\" will be close in vector space\n",
    "3. **Enable similarity calculations** - We can use vector operations to find similar content\n",
    "\n",
    "### How Embedding Models Work (Simplified)\n",
    "\n",
    "1. **Neural networks learn from vast amounts of text** - They read billions of documents\n",
    "2. **They identify patterns in how words are used together** - Words that appear in similar contexts get similar vectors\n",
    "3. **The resulting model can convert any text to a fixed-length vector** - Usually hundreds of dimensions\n",
    "\n",
    "### Remarkable Properties of Embeddings\n",
    "\n",
    "- **Semantic relationships**: Words with similar meanings have vectors that are close together\n",
    "- **Analogy solving**: Famous example: \"king\" - \"man\" + \"woman\" â‰ˆ \"queen\"\n",
    "- **Cross-lingual capabilities**: Some models can relate similar concepts across languages\n",
    "\n",
    "Let's see a practical example of generating text embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a6fcd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of text embeddings with visualization\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Load a pre-trained embedding model\n",
    "model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "model = HuggingFaceEmbeddings(model_name=model_name)\n",
    "\n",
    "# Let's define some example sentences with semantic relationships\n",
    "sentences = [\n",
    "    \"Dogs are wonderful pets.\",\n",
    "    \"I love my canine companion.\",  # Similar meaning to the first sentence\n",
    "    \"Cats make great household pets.\",  # Related but different animal\n",
    "    \"Artificial intelligence is changing technology.\",  # Completely different topic\n",
    "    \"Machine learning algorithms can solve complex problems.\",  # Similar to AI sentence\n",
    "    \"The weather is beautiful today.\",  # Unrelated topic\n",
    "]\n",
    "\n",
    "# Generate embeddings for each sentence\n",
    "embeddings = model.embed_documents(sentences)\n",
    "\n",
    "# Basic embedding information\n",
    "print(f\"Shape of a single embedding: {len(embeddings[0])}\")\n",
    "print(f\"Number of dimensions: {len(embeddings)}\")\n",
    "print(f\"\\nFirst 5 values of first embedding: \\n{embeddings[0][:5]}\")\n",
    "print(\n",
    "    \"\\nNotice these are just numbers - the meaning is distributed across all dimensions!\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1756623a",
   "metadata": {},
   "source": [
    "## Cosine Similarity\n",
    "\n",
    "Cosine similarity measures the angle between two vectors, providing a metric for similarity. This is a fundamental concept in semantic search, so let's break it down for beginners.\n",
    "\n",
    "### What is Cosine Similarity?\n",
    "\n",
    "Cosine similarity measures how similar two vectors are by calculating the cosine of the angle between them. It ranges from -1 (completely opposite) to 1 (exactly the same), with 0 indicating no relationship.\n",
    "\n",
    "### Cosine Similarity Formula\n",
    "\n",
    "Mathematically, cosine similarity is calculated as (**feel free to skip this section**):\n",
    "\n",
    "$$\\text{Cosine Similarity} = \\frac{A \\cdot B}{||A|| \\times ||B||} = \\frac{\\sum_{i=1}^{n} A_i B_i}{\\sqrt{\\sum_{i=1}^{n} A_i^2} \\times \\sqrt{\\sum_{i=1}^{n} B_i^2}}$$\n",
    "\n",
    "Where:\n",
    "- $A \\cdot B$ is the dot product of vectors A and B\n",
    "- $||A||$ and $||B||$ are the magnitudes (lengths) of vectors A and B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad0cbce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate similarity matrix between all sentence pairs\n",
    "similarity_matrix = cosine_similarity(embeddings)\n",
    "\n",
    "# Display similarities\n",
    "print(\"\\n==== Semantic Similarity Between Sentences ====\\n\")\n",
    "for i, sentence1 in enumerate(sentences):\n",
    "    print(f\"Sentence {i + 1}: {sentence1}\")\n",
    "\n",
    "print(\"\\nSimilarity Matrix (Cosine Similarity):\")\n",
    "print(np.round(similarity_matrix, 2))\n",
    "\n",
    "# Let's highlight specific relationships\n",
    "print(\"\\nObserve the similarities:\\n\")\n",
    "print(\n",
    "    f\"Pet sentences (1 & 2): {similarity_matrix[0][1]:.2f} - High similarity as they're about the same concept\"\n",
    ")\n",
    "print(\n",
    "    f\"Pet sentences (1 & 3): {similarity_matrix[0][2]:.2f} - Moderate similarity (both about pets, but different animals)\"\n",
    ")\n",
    "print(\n",
    "    f\"Unrelated topics (1 & 6): {similarity_matrix[0][5]:.2f} - Low similarity as they're about unrelated topics\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Databases\n",
    "\n",
    "To compare texts we need to firstly transtale them to embeddings (vectors). Then we can simply find the similar ones using cosine similary. \n",
    "\n",
    "To store vector we use Vector Datastores, which are optimized for vector calculations.\n",
    "\n",
    "### What are Vector Databases?\n",
    "\n",
    "Vector databases are purpose-built systems that:\n",
    "- Store high-dimensional vectors (like our text embeddings)\n",
    "- Create special indexes for fast similarity searches\n",
    "- Allow retrieval of the most similar vectors to a query vector\n",
    "- Store additional metadata alongside the vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e94fa4",
   "metadata": {},
   "source": [
    "## Pinecone Implementation\n",
    "\n",
    "Pinecone is a popular managed vector database service that provides scalable and efficient vector search capabilities for production environments. Let's explore how it works and how to implement it:\n",
    "\n",
    "### What is Pinecone?\n",
    "\n",
    "Pinecone is a cloud-based vector database that specializes in:\n",
    "- **Storing and searching large-scale vector embeddings** - Billions of vectors at production scale\n",
    "- **Low-latency retrieval** - Sub-second query times even with large datasets\n",
    "- **Horizontal scaling** - Easy to grow as your data volume increases\n",
    "- **Managed infrastructure** - No need to manage your own servers or instances\n",
    "\n",
    "### How Pinecone Works\n",
    "\n",
    "At a high level, Pinecone follows these steps:\n",
    "\n",
    "1. **Create an index** - A specialized data structure for storing vectors\n",
    "2. **Upload vectors** - Along with optional metadata (like the original text or other attributes)\n",
    "3. **Query the index** - Find the most similar vectors to a query vector\n",
    "4. **Retrieve results** - Get back the most similar vectors and their metadata\n",
    "\n",
    "### Key Concepts in Pinecone\n",
    "\n",
    "- **Index**: A collection of vectors with a specific dimensionality and distance metric\n",
    "- **Vector**: An array of floating-point numbers (your embeddings)\n",
    "- **Metadata**: Additional information about each vector (e.g., the original text, URL, category)\n",
    "- **Namespace**: An optional way to partition your vectors for better organization\n",
    "- **Query**: A request to find vectors similar to a given vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f08ad139",
   "metadata": {},
   "source": [
    "## Pinecone setup\n",
    "\n",
    "Please visit `http://pinecone.io`, log in with your email and generate api key. **Share this api key in your teammebers, so all of you could access the same database.**\n",
    "\n",
    "Create `.env` file and paste there Pinecone API key:\n",
    "\n",
    "`PINECONE_API_KEY=your_api_key`\n",
    "\n",
    "Then continue the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25662db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables (particularly PINECONE_API_KEY)\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852f8519",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Example of Pinecone implementation\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "\n",
    "# Initialize Pinecone with your API key\n",
    "pc = Pinecone(api_key=os.getenv(\"PINECONE_API_KEY\"))\n",
    "\n",
    "\n",
    "# Function to create and populate an index\n",
    "def setup_pinecone_index(index_name, dimension=768):\n",
    "    \"\"\"\n",
    "    Create and populate a Pinecone index.\n",
    "    args:\n",
    "        index_name (str): Name of the index to create.\n",
    "        dimension (int): Dimension of the embeddings model. Use HuggingFace description of the model to find the dimension.\n",
    "    \"\"\"\n",
    "    # Check if the index already exists\n",
    "    existing_indexes = pc.list_indexes()\n",
    "    if index_name not in [\n",
    "        existing_index.get(\"name\") for existing_index in existing_indexes\n",
    "    ]:\n",
    "        # Create a new index\n",
    "        pc.create_index(\n",
    "            name=index_name,\n",
    "            dimension=dimension,\n",
    "            metric=\"cosine\",\n",
    "            spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\"),\n",
    "        )\n",
    "\n",
    "    # Connect to the index\n",
    "    index = pc.Index(index_name)\n",
    "\n",
    "    return index\n",
    "\n",
    "\n",
    "index = setup_pinecone_index(\"test2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37fb3dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_pinecone import PineconeVectorStore\n",
    "\n",
    "vector_store = PineconeVectorStore(index=index, embedding=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5a17c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "from uuid import uuid4\n",
    "\n",
    "document_1 = Document(\n",
    "    page_content=\"I had chocolate chip pancakes and scrambled eggs for breakfast this morning.\",\n",
    "    metadata={\"source\": \"tweet\"},\n",
    ")\n",
    "\n",
    "document_2 = Document(\n",
    "    page_content=\"The weather forecast for tomorrow is cloudy and overcast, with a high of 62 degrees.\",\n",
    "    metadata={\"source\": \"news\"},\n",
    ")\n",
    "\n",
    "document_3 = Document(\n",
    "    page_content=\"Building an exciting new project with LangChain - come check it out!\",\n",
    "    metadata={\"source\": \"tweet\"},\n",
    ")\n",
    "\n",
    "document_4 = Document(\n",
    "    page_content=\"Robbers broke into the city bank and stole $1 million in cash.\",\n",
    "    metadata={\"source\": \"news\"},\n",
    ")\n",
    "\n",
    "document_5 = Document(\n",
    "    page_content=\"Wow! That was an amazing movie. I can't wait to see it again.\",\n",
    "    metadata={\"source\": \"tweet\"},\n",
    ")\n",
    "\n",
    "document_6 = Document(\n",
    "    page_content=\"Is the new iPhone worth the price? Read this review to find out.\",\n",
    "    metadata={\"source\": \"website\"},\n",
    ")\n",
    "\n",
    "document_7 = Document(\n",
    "    page_content=\"The top 10 soccer players in the world right now.\",\n",
    "    metadata={\"source\": \"website\"},\n",
    ")\n",
    "\n",
    "document_8 = Document(\n",
    "    page_content=\"LangGraph is the best framework for building stateful, agentic applications!\",\n",
    "    metadata={\"source\": \"tweet\"},\n",
    ")\n",
    "\n",
    "document_9 = Document(\n",
    "    page_content=\"The stock market is down 500 points today due to fears of a recession.\",\n",
    "    metadata={\"source\": \"news\"},\n",
    ")\n",
    "\n",
    "document_10 = Document(\n",
    "    page_content=\"I have a bad feeling I am going to get deleted :(\",\n",
    "    metadata={\"source\": \"tweet\"},\n",
    ")\n",
    "\n",
    "documents = [\n",
    "    document_1,\n",
    "    document_2,\n",
    "    document_3,\n",
    "    document_4,\n",
    "    document_5,\n",
    "    document_6,\n",
    "    document_7,\n",
    "    document_8,\n",
    "    document_9,\n",
    "    document_10,\n",
    "]\n",
    "uuids = [str(uuid4()) for _ in range(len(documents))]\n",
    "vector_store.add_documents(documents=documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ac5d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data from csv file\n",
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "df = pd.read_csv(\"../data/example_csv.csv\")\n",
    "# Display the first few rows of the DataFrame\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361521d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store.add_documents(\n",
    "    documents=[\n",
    "        Document(page_content=row[\"text\"], metadata={\"source\": row[\"source\"]})\n",
    "        for _, row in df.iterrows()\n",
    "    ],\n",
    "    ids=[str(uuid4()) for _ in range(len(df))],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513a07b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = vector_store.similarity_search(\n",
    "    \"LangChain provides abstractions to make working with LLMs easy\",\n",
    "    k=2,\n",
    "    filter={\"source\": \"tweet\"},\n",
    ")\n",
    "for res in results:\n",
    "    print(f\"* {res.page_content} [{res.metadata}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60386ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = vector_store.similarity_search_with_score(\n",
    "    \"Will it be hot tomorrow?\", k=1, filter={\"source\": \"news\"}\n",
    ")\n",
    "for res, score in results:\n",
    "    print(f\"* [SIM={score:3f}] {res.page_content} [{res.metadata}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b86ffe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = vector_store.similarity_search_with_score(\"Welcome in excel world?\", k=4)\n",
    "for res, score in results:\n",
    "    print(f\"* [SIM={score:3f}] {res.page_content} [{res.metadata}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion: Putting It All Together\n",
    "\n",
    "### Key Concepts Reviewed\n",
    "\n",
    "In this notebook, we've explored the fundamental concepts behind semantic search:\n",
    "\n",
    "1. **Vectors**: The mathematical backbone of semantic search\n",
    "   - Lists of numbers representing points in multi-dimensional space\n",
    "   - Allow us to convert text, images, and other data into mathematical representations\n",
    "   - Enable similarity calculations between different pieces of information\n",
    "\n",
    "2. **Text Embeddings**: How we transform text into vectors\n",
    "   - Neural networks create high-dimensional vectors that capture semantic meaning\n",
    "   - Similar concepts have similar vector representations, regardless of the exact words used\n",
    "   - Modern embedding models encode context and nuance, not just keywords\n",
    "\n",
    "3. **Cosine Similarity**: How we measure similarity between vectors\n",
    "   - Based on the angle between vectors, not their magnitude\n",
    "   - Values close to 1 indicate high similarity; values close to 0 indicate low similarity\n",
    "   - Works well in high-dimensional spaces used for text embeddings\n",
    "\n",
    "4. **Vector Databases**: How we store and retrieve vectors efficiently\n",
    "   - Specialized for handling high-dimensional vector data\n",
    "   - Enable fast similarity search across millions or billions of vectors\n",
    "   - Examples include Pinecone, FAISS, and Milvus\n",
    "\n",
    "5. **Semantic Search**: Understanding meaning, handles synonyms and related concepts\n",
    "\n",
    "### Semantic Search Workflow\n",
    "\n",
    "The typical semantic search workflow consists of these steps:\n",
    "\n",
    "1. **Indexing Phase**:\n",
    "   - Convert documents/content into vector embeddings using a neural network\n",
    "   - Store these embeddings in a vector database (optionally with metadata)\n",
    "   - Create efficient indexes for fast retrieval\n",
    "\n",
    "2. **Query Phase**:\n",
    "   - Convert the search query into a vector embedding using the same model\n",
    "   - Find the closest vectors to the query vector using cosine similarity\n",
    "   - Return the documents/content associated with the most similar vectors\n",
    "\n",
    "   \n",
    "By understanding these core concepts, you now have the foundation to implement and work with semantic search systems in various applications. The power of semantic search comes from its ability to understand meaning rather than just matching keywords, making it an essential technology for processing and retrieving information in our increasingly data-rich world."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3efe4af2",
   "metadata": {},
   "source": [
    "# Next steps\n",
    "\n",
    "\n",
    "\n",
    "Now you can link materials found by LLM with matching one from ecoinvent. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20cb1604",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "This notebook marks warehouse data management exercise. The primary goal here is Exploratory Data Analysis (EDA) and clean the data. We will load the provided `warehouse_data_messy.csv` dataset and perform an initial investigation to understand its structure, identify potential data quality issues, and get a general feel for the data distributions.\n",
    "\n",
    "Next we will address the data quality issues identified, such as missing values, incorrect data types, inconsistent formatting, potential duplicates, and outliers.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c34a16fa",
   "metadata": {},
   "source": [
    "# 1. Setup and Data Loading\n",
    "\n",
    "First, we import the necessary library (pandas) and load the dataset from the CSV file into a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9243f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset from the specified CSV file path\n",
    "file_path = \"../data/warehouse_data_messy.csv\"\n",
    "df = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef51f31",
   "metadata": {},
   "source": [
    "# 2. Initial Data Inspection\n",
    "\n",
    "Let's start with some basic pandas functions to get an overview of the DataFrame\n",
    "\n",
    "## 2.1. Dataframe Structure and Types\n",
    "The `info()` method provides a concise summary of the DataFrame, including the number of non-null entries for each column and their data types (Dtypes). This helps identify columns with missing data and incorrect data types early on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b10aa1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display DataFrame info: column names, non-null counts, and data types\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a26bc01",
   "metadata": {},
   "source": [
    "## 2.2. Fetch rows\n",
    "\n",
    "The `head()` method displays the first few rows of the DataFrame (default is 5). This gives a quick look at the actual data values and helps spot obvious issues like inconsistent formatting or placeholder values.\n",
    "\n",
    "To fetch last rows use method `tail()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd873963",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the first 5 rows of the DataFrame\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2bd8fb8",
   "metadata": {},
   "source": [
    "## 2.3. Descriptive Statistics\n",
    "\n",
    "The `describe()` method generates descriptive statistics for numerical columns (count, mean, std, min, max, quartiles). Using include='all' extends this to object/categorical columns, showing count, unique values, top value, and frequency.\n",
    "\n",
    "**Note:** This might raise errors or produce unexpected results if numerical columns contain non-numeric data, which is common in messy datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce10dac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate descriptive statistics for all columns\n",
    "df.describe(include=\"all\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d87d9ac",
   "metadata": {},
   "source": [
    "# 3. Automated Data Profiling\n",
    "\n",
    "For a more in-depth and automated overview we use the `ydata-profiling` library. It generates an interactive report summarizing various aspects of the data, including distributions, correlations, missing values, and potential quality issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88351f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the ProfileReport class\n",
    "from ydata_profiling import ProfileReport\n",
    "\n",
    "# Create a profile report instance\n",
    "profile = ProfileReport(df, title=\"Raw Warehouse Data Profile\", explorative=True)\n",
    "profile.to_notebook_iframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bedcf5db",
   "metadata": {},
   "source": [
    "Start with exploring Overview section of the report. It let you quickly understand the data. \n",
    "In this step you should notice some issues with data.\n",
    "- 11 duplicated records,\n",
    "- 99 missing values.\n",
    "\n",
    "Then familirize with each Variable in the dataset. It let you understand the content of the selected column.\n",
    "For example, you can see that `Product ID` has no missing values, but `location` has 18. `Quantity` and `Price` has incorrect column type. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa7a632",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can save the report to an HTML file\n",
    "profile_output_path = \"warehouse_data_profile.html\"\n",
    "profile.to_file(profile_output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74176519",
   "metadata": {},
   "source": [
    "# 4. Data cleaning \n",
    "\n",
    "## 4.1. Handling Missing Values\n",
    "\n",
    "Missing data can occur in various forms (standard NaN, empty strings, custom placeholders). We need to identify and address them appropriately."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcde1927",
   "metadata": {},
   "source": [
    "### 4.1.1. Standard Missing Values\n",
    "\n",
    "You can read missing values from above report or calculate it with Pandas `isnull().sum()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3283efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count standard NaN values per column\n",
    "standard_missing = df.isnull().sum()\n",
    "print(\"Standard Missing Values (NaN):\\n\", standard_missing[standard_missing > 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a089d2db",
   "metadata": {},
   "source": [
    "### 4.1.2. Non-standard Missing Values\n",
    "\n",
    "Often, missing data is represented by strings like \"unknown\", \"N/A\", \"--\", etc. We need to define these patterns and search for them, typically in object-type columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48bb950b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define patterns representing missing data\n",
    "missing_patterns = [\n",
    "    \"--\",\n",
    "    \"N/N\",\n",
    "    \"unknown\",\n",
    "    \"?\",\n",
    "    \"None\",\n",
    "    \"NA\",\n",
    "    \"\",\n",
    "    \"not available\",\n",
    "    \"null\",\n",
    "]  # Added 'not available' and 'null'\n",
    "\n",
    "# Create a boolean mask for non-standard missing values across the dataframe\n",
    "non_standard_missing_mask = df.map(lambda x: str(x).strip() in missing_patterns)\n",
    "\n",
    "# Count non-standard missing values per column\n",
    "non_standard_missing_counts = non_standard_missing_mask.sum()\n",
    "\n",
    "print(\n",
    "    \"Non-Standard Missing Values (Counts):\\n\",\n",
    "    non_standard_missing_counts[non_standard_missing_counts > 0],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b6e9b7",
   "metadata": {},
   "source": [
    "Now, replace the identified non-standard missing values with the standard np.nan. This makes subsequent handling easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39be671e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Replace non-standard patterns with np.nan\n",
    "# We iterate through columns and apply replacement only where the mask is True\n",
    "for col in df.columns:\n",
    "    # Check if the column actually had any non-standard missing values to replace\n",
    "    if non_standard_missing_counts.get(col, 0) > 0:\n",
    "        df.loc[non_standard_missing_mask[col], col] = np.nan\n",
    "\n",
    "# Verify total missing values now (should include both original NaN and replaced patterns)\n",
    "total_missing = df.isnull().sum()\n",
    "print(\n",
    "    \"Total Missing Values (NaN after consolidation):\\n\",\n",
    "    total_missing[total_missing > 0],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db11aa00",
   "metadata": {},
   "source": [
    "### Imputation \n",
    "\n",
    "We need to decide how to fill the NaN values. Common strategies include:\n",
    "\n",
    "- **Mean/median/mode or most common value imputation**: Simple, but can distort variance and correlations,\n",
    "- **Forward/Backward fill**: Useful for time-series or ordered data,\n",
    "- **Constant value**: Filling with a specific value (like 0, \"Unknown\"),\n",
    "- **Dropping**: Removing rows or columns with missing data (use cautiously),\n",
    "- **Model-based imputation (e.g., IterativeImputer)**: More complex, uses other features to predict missing values.\n",
    "\n",
    "Strategy Chosen Here:\n",
    "\n",
    "- `Quantity`: Impute with 0 (assuming missing means zero stock, but review this assumption).\n",
    "- `Price`: Impute with the median, as it's less sensitive to outliers than the mean (we need to convert to numeric first).\n",
    "- `Location`: Impute with a placeholder like \"Unknown\".\n",
    "- `Status`: Impute with the mode (most frequent value).\n",
    "- `Last Restocked`: Impute with a specific placeholder date or use ffill/bfill if ordering makes sense (we'll use a placeholder here after date conversion)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c64e57",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Imputing categorical columns (Location, Status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17154721",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute Location with 'Unknown'\n",
    "df[\"Location\"] = df[\"Location\"].fillna(\"Unknown\")\n",
    "print(\"Imputed 'Location' missing values with 'Unknown'.\")\n",
    "\n",
    "# Impute Status with the mode\n",
    "status_mode = df[\"Status\"].mode()[0]  # mode() returns a Series, get the first element\n",
    "df[\"Status\"] = df[\"Status\"].fillna(status_mode)\n",
    "print(f\"Imputed 'Status' missing values with mode: '{status_mode}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5487230a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Imputing numerical columns (Quantity, Price)\n",
    "\n",
    "First, convert these columns to numeric types, coercing errors. Values that cannot be converted (like remaining non-standard patterns missed earlier, or text) will become `NaN`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13422aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Quantity and Price to numeric, errors become NaN\n",
    "df[\"Quantity\"] = pd.to_numeric(df[\"Quantity\"], errors=\"coerce\")\n",
    "df[\"Price\"] = pd.to_numeric(df[\"Price\"], errors=\"coerce\")\n",
    "\n",
    "# Check for any new NaNs introduced by coercion\n",
    "print(\n",
    "    \"Missing values after numeric conversion:\\n\",\n",
    "    df[[\"Quantity\", \"Price\"]].isnull().sum(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830c3911",
   "metadata": {},
   "source": [
    "#### Impute Price with the median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482fd302",
   "metadata": {},
   "outputs": [],
   "source": [
    "price_median = df[\"Price\"].median()\n",
    "df[\"Price\"] = df[\"Price\"].fillna(price_median)\n",
    "print(f\"Imputed 'Price' missing values with median: {price_median:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb8bc99",
   "metadata": {},
   "source": [
    "#### Impute Quantity with MICE algorithm\n",
    "\n",
    "MICE (Multiple Imputation by Chained Equations) is an AI method for handling missing data. That works by:\n",
    "1. Initially filling missing values with simple mean/median imputation. This is a temporary replacement. At the end of this step, there should be no missing values.\n",
    "2. For the specific column you want to impute, eg: columm A alone, change the imputed value back to missing.\n",
    "3. Now, build a regression model to predict A using (B and C) as predictors. For this model, only the non-missing rows of A are included. So, A is the response, while, B and C are predictors. Use this model to predict the missing values in A.\n",
    "4. Repeat steps 2-3 for columns B and C as well.\n",
    "\n",
    "MICE works with only numerical features. What can we do in this situation?\n",
    "We can transform categorical features (such as Category, Warehouse, Supplier) to numeric using label encoding. Each unique value will be transformed to unique numeric identifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17914a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "category_numeric = le.fit_transform(df[\"Category\"])\n",
    "print(\n",
    "    \"Categories and their numeric encodings:\\n\",\n",
    "    dict(zip(le.classes_, le.transform(le.classes_))),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db94ec08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert rest of the categorical columns to numeric\n",
    "# Skip Product ID as it is unique for each product and not useful for predicting missing values\n",
    "\n",
    "features = [\n",
    "    \"Product Name\",\n",
    "    \"Category\",\n",
    "    \"Warehouse\",\n",
    "    \"Location\",\n",
    "    \"Price\",\n",
    "    \"Supplier\",\n",
    "    \"Status\",\n",
    "    \"Quantity\",\n",
    "]\n",
    "\n",
    "# Create a copy of the dataframe with selected features\n",
    "df_for_imputation = df[features].copy()\n",
    "\n",
    "# Convert categorical variables to numeric using label encoding\n",
    "# we saved the label encoder instance to reuse it for backward transformation\n",
    "encoders = {}\n",
    "for col in df_for_imputation:\n",
    "    if df_for_imputation[col].dtype == \"object\":\n",
    "        le = LabelEncoder()\n",
    "        encoders[col] = le\n",
    "        df_for_imputation[col] = le.fit_transform(df_for_imputation[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1fa13c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "# Define imputer\n",
    "imputer = IterativeImputer()\n",
    "\n",
    "# fit on the dataset\n",
    "imputer.fit(df_for_imputation)\n",
    "\n",
    "imputed_array = imputer.transform(df_for_imputation)\n",
    "df_imputed = pd.DataFrame(imputed_array, columns=df_for_imputation.columns)\n",
    "\n",
    "# use the saved encoders to convert back to original categorical values\n",
    "for col, le in encoders.items():\n",
    "    df_imputed[col] = le.inverse_transform(df_imputed[col].astype(int))\n",
    "\n",
    "# check number of missing values in Quantity column\n",
    "print(\n",
    "    \"Missing values in 'Quantity' after imputation:\\n\",\n",
    "    df_imputed[\"Quantity\"].isnull().sum(),\n",
    ")\n",
    "\n",
    "# copy quanity column to original dataframe\n",
    "df[\"Quantity\"] = df_imputed[\"Quantity\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee2bb9e",
   "metadata": {},
   "source": [
    "### Handling Last Restocked (Date Column)\n",
    "\n",
    "Convert to datetime objects, handling potential errors and different formats. Then impute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3f4bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'Last Restocked' to datetime, coercing errors\n",
    "# The 'infer_datetime_format=True' can help with mixed formats, but might be slow.\n",
    "# For truly mixed/complex formats, might need custom parsing logic.\n",
    "df[\"Last Restocked\"] = pd.to_datetime(df[\"Last Restocked\"], errors=\"coerce\")\n",
    "\n",
    "# Check for NaNs introduced by conversion or already present\n",
    "print(\n",
    "    f\"Missing 'Last Restocked' values before imputation: {df['Last Restocked'].isnull().sum()}\\n\"\n",
    ")\n",
    "\n",
    "# Imputation Strategy: Fill with a placeholder date (e.g., earliest date or a sentinel value)\n",
    "# Or use ffill/bfill if data is chronologically ordered (not assumed here).\n",
    "# Let's use the overall minimum date found in the column as placeholder.\n",
    "min_date = df[\"Last Restocked\"].min()\n",
    "df[\"Last Restocked\"] = df[\"Last Restocked\"].fillna(min_date)\n",
    "print(\n",
    "    f\"Imputed 'Last Restocked' missing values with min date: {min_date}. Note: Review if this is appropriate.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e12b62f",
   "metadata": {},
   "source": [
    "### Final Check for Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d28263a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify that there are no more missing values\n",
    "missing_after_imputation = df.isnull().sum().sum()\n",
    "if missing_after_imputation == 0:\n",
    "    print(\"Success: No missing values remaining in the dataset.\")\n",
    "else:\n",
    "    print(f\"Warning: {missing_after_imputation} missing values still present.\")\n",
    "    print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f84ec40c",
   "metadata": {},
   "source": [
    "## 4.2 Correcting data types \n",
    "\n",
    "Ensure all columns have the appropriate data types for analysis.\n",
    "\n",
    "- `Product ID`: Should ideally be string/object if not used mathematically, to avoid accidental calculations. Let's keep as int64 for now, assuming it's a simple ID.\n",
    "- `Quantity`: Should be integer type now that it's cleaned and imputed.\n",
    "- `Price`: Should be float type.\n",
    "- `Last Restocked`: Should be datetime type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdbc85aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Quantity to integer (assuming whole units)\n",
    "# Use Int64 (capital I) to allow for potential NaNs if any were missed, though we aimed to fill all.\n",
    "try:\n",
    "    df[\"Quantity\"] = df[\"Quantity\"].astype(\"Int64\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not convert Quantity to Int64: {e}. Check for non-integer values.\")\n",
    "    # Fallback or further investigation needed if conversion fails\n",
    "\n",
    "# Price is already float (implicitly from to_numeric and median)\n",
    "# Last Restocked is already datetime\n",
    "\n",
    "# Display final data types\n",
    "print(\"\\nFinal Data Types:\\n\", df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ee1f3e",
   "metadata": {},
   "source": [
    "## 4.3. Handling inconsistent formatting\n",
    "\n",
    "Clean up string columns by removing leading/trailing whitespace and extra quotes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0983aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select object type columns (strings)\n",
    "string_columns = df.select_dtypes(include=[\"object\"]).columns\n",
    "\n",
    "for col in string_columns:\n",
    "    # Remove leading/trailing whitespace\n",
    "    df[col] = df[col].str.strip()\n",
    "    # Remove potential extra quotes (be careful not to remove legitimate internal quotes if any)\n",
    "    df[col] = df[col].str.replace(\n",
    "        '^\"|\"$', \"\", regex=True\n",
    "    )  # Removes quotes only at start/end\n",
    "\n",
    "print(\"String columns cleaned (whitespace and edge quotes removed).\")\n",
    "# Display some examples from a potentially affected column like Product Name\n",
    "print(\"\\nExample cleaned 'Product Name':\")\n",
    "print(df[\"Product Name\"].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f05026",
   "metadata": {},
   "source": [
    "## 4.4 Handling duplicates\n",
    "\n",
    "Identify and remove duplicate rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0e9aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect duplicate rows\n",
    "duplicates = df.duplicated()\n",
    "num_duplicates = duplicates.sum()\n",
    "print(f\"Number of duplicate rows found: {num_duplicates}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d6db0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates(inplace=True)\n",
    "print(\"Duplicate rows removed.\")\n",
    "# Reset index after dropping rows\n",
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e115d359",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check again if there are duplicated\n",
    "duplicates = df.duplicated()\n",
    "num_duplicates = duplicates.sum()\n",
    "print(f\"Number of duplicate rows found: {num_duplicates}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d30233a6",
   "metadata": {},
   "source": [
    "## 4.3. Handling outliers\n",
    "\n",
    "Outliers are extreme values that can skew analysis. We'll focus on Quantity and Price.\n",
    "\n",
    "**Strategy:** Identify outliers using the Interquartile Range (IQR) method and decide whether to cap them (replace with a boundary value) or remove them. We'll visualize first, then cap."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a7fa42",
   "metadata": {},
   "source": [
    "### Visualize Potential Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbec8e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Create box plots to visualize distributions and potential outliers\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.boxplot(y=df[\"Quantity\"])\n",
    "plt.title(\"Box Plot of Quantity\")\n",
    "plt.ylabel(\"Quantity\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.boxplot(y=df[\"Price\"])\n",
    "plt.title(\"Box Plot of Price\")\n",
    "plt.ylabel(\"Price\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd60810",
   "metadata": {},
   "source": [
    "The outliers are shown in the boxplots as dots above or below the upper/lower whisker. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a59cd412",
   "metadata": {},
   "source": [
    "### Capping Outliers using IQR\n",
    "We'll cap values below Q1 - 1.5IQR or above Q3 + 1.5IQR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c906f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cap_outliers(series):\n",
    "    Q1 = series.quantile(0.25)\n",
    "    Q3 = series.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "    # Cap values\n",
    "    capped_series = series.clip(lower=lower_bound, upper=upper_bound)\n",
    "    num_capped = (series < lower_bound).sum() + (series > upper_bound).sum()\n",
    "    print(\n",
    "        f\"Capped {num_capped} outliers in '{series.name}' (Lower: {lower_bound:.2f}, Upper: {upper_bound:.2f}).\"\n",
    "    )\n",
    "    return capped_series\n",
    "\n",
    "\n",
    "# Apply capping to Quantity and Price\n",
    "df[\"Quantity\"] = cap_outliers(df[\"Quantity\"])\n",
    "df[\"Price\"] = cap_outliers(df[\"Price\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aac8460",
   "metadata": {},
   "source": [
    "### Visualize After Capping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61b1d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize again after capping\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.boxplot(y=df[\"Quantity\"])\n",
    "plt.title(\"Box Plot of Quantity (After Capping)\")\n",
    "plt.ylabel(\"Quantity\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.boxplot(y=df[\"Price\"])\n",
    "plt.title(\"Box Plot of Price (After Capping)\")\n",
    "plt.ylabel(\"Price\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216a94cb",
   "metadata": {},
   "source": [
    "# 5. Final Review and Saving Cleaned Data\n",
    "\n",
    "Let's review the first few rows and the info of the cleaned DataFrame before saving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77ad6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Cleaned DataFrame Info:\")\n",
    "df.info()\n",
    "\n",
    "print(\"\\nFirst 5 rows of Cleaned DataFrame:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce5b70fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regenerate report on cleaned dataset to be sure that all issues are handled\n",
    "\n",
    "profile = ProfileReport(df, title=\"Raw Warehouse Data Profile\", explorative=True)\n",
    "profile.to_notebook_iframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f964c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sdv.metadata import SingleTableMetadata\n",
    "\n",
    "metadata = SingleTableMetadata()\n",
    "metadata.detect_from_dataframe(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3766fee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Synthetic Data Generation\n",
    "\n",
    "from sdv.single_table import CTGANSynthesizer\n",
    "\n",
    "synthesizer = CTGANSynthesizer(metadata)\n",
    "synthesizer.fit(df)\n",
    "\n",
    "synthetic_data = synthesizer.sample(num_rows=50)\n",
    "synthetic_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "520d05b4",
   "metadata": {},
   "source": [
    "### Evaluating Real vs. Synthetic Data\n",
    "SDV has built-in functions for evaluating the synthetic data and getting more insight.\n",
    "\n",
    "As a first step, we can run a diagnostic to ensure that the data is valid. SDV's diagnostic performs some basic checks such as:\n",
    "\n",
    "- All primary keys must be unique\n",
    "- Continuous values must adhere to the min/max of the real data\n",
    "- Discrete columns must have the same categories as the real data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80acfbcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sdv.evaluation.single_table import run_diagnostic\n",
    "\n",
    "diagnostic = run_diagnostic(\n",
    "    real_data=df, synthetic_data=synthetic_data, metadata=metadata\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920b7527",
   "metadata": {},
   "source": [
    "The score is 100%, indicating that the data is fully valid."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be758ceb",
   "metadata": {},
   "source": [
    "We can also measure the data quality or the statistical similarity between the real and synthetic data. This value may vary anywhere from 0 to 100%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbfb4d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sdv.evaluation.single_table import evaluate_quality\n",
    "\n",
    "quality_report = evaluate_quality(df, synthetic_data, metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a46203cf",
   "metadata": {},
   "source": [
    "According to the score, the synthetic data is about 71% similar to the real data in terms of statistical similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1532515",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Price\"].plot.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2828472",
   "metadata": {},
   "outputs": [],
   "source": [
    "synthetic_data[\"Price\"].plot.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832903df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the cleaned DataFrame to a new CSV file\n",
    "df.to_csv(\"../data/warehouse_data_cleaned.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d8502e2",
   "metadata": {},
   "source": [
    "# 6. Conclusion\n",
    "In this notebook, we performed essential data cleaning tasks on the warehouse inventory data. We addressed:\n",
    "\n",
    "- Standard and non-standard missing values through identification and imputation,\n",
    "- Incorrect data types by converting columns like Quantity, Price, and Last Restocked,\n",
    "- Inconsistent string formatting by trimming whitespace and removing extraneous quotes,\n",
    "- Duplicated records by identifying and removing them,\n",
    "- Outliers by visualizing and capping them using the IQR method,\n",
    "- Missing data imputation,\n",
    "- Synthetic Data Generation.\n",
    "The resulting DataFrame (df) is now cleaned and stored in `warehouse_data_cleaned.csv`. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
